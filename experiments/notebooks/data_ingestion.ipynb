{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "282c8687",
   "metadata": {},
   "source": [
    "**DATA INGESTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74556d37",
   "metadata": {},
   "source": [
    "1. Go through the url workbook\n",
    "2. Ascertain that the number of rows and columns are as expected\n",
    "3. Implement a customer logger\n",
    "4. Load the pd.concat dataset back into the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed723d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Project root: c:\\Users\\juliu\\OneDrive\\Desktop\\Projects\\Soca-Scores\n",
      "ğŸ“ Data directory: c:\\Users\\juliu\\OneDrive\\Desktop\\Projects\\Soca-Scores\\datasets\n",
      "ğŸ“ Common data directory: c:\\Users\\juliu\\OneDrive\\Desktop\\Projects\\Soca-Scores\\datasets\\common_data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== Defining the data directories ====#\n",
    "PROJECT_ROOT = Path().absolute().parent.parent\n",
    "DATA_DIR = PROJECT_ROOT / \"datasets\" \n",
    "COMMON_DATA_DIR = DATA_DIR / \"common_data\"\n",
    "\n",
    "print(f\"ğŸ“ Project root: {PROJECT_ROOT}\")\n",
    "print(f\"ğŸ“ Data directory: {DATA_DIR}\")\n",
    "print(f\"ğŸ“ Common data directory: {COMMON_DATA_DIR}\")\n",
    "\n",
    "# == Dont forget the clear from the github commits == #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4ca06b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found file: c:\\Users\\juliu\\OneDrive\\Desktop\\Projects\\Soca-Scores\\datasets\\common_data\\english_league_data_url.csv\n",
      "ğŸ“ŠFile has been loaded successfully. The url dataset has 15 columns and 3 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Loading the url datasets ====#\n",
    "\n",
    "csv_file = COMMON_DATA_DIR / \"english_league_data_url.csv\"\n",
    "try:\n",
    "    if csv_file.exists():\n",
    "        print(f\"âœ… Found file: {csv_file}\")\n",
    "        #proceed to load it\n",
    "        urls = pd.read_csv(csv_file)\n",
    "        print(f'ğŸ“ŠFile has been loaded successfully. The url dataset has {urls.shape[0]} columns and {urls.shape[1]} rows')\n",
    "    else:\n",
    "        print(f\"âŒ File not found: {csv_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Operation unsuccessful. Reason : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc584c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports completed!\n",
      "ğŸ“ Project root: c:\\Users\\juliu\\OneDrive\\Desktop\\Projects\\Soca-Scores\n",
      "ğŸ“ Data directory: c:\\Users\\juliu\\OneDrive\\Desktop\\Projects\\Soca-Scores\\datasets\n",
      "ğŸ“ Common data directory: c:\\Users\\juliu\\OneDrive\\Desktop\\Projects\\Soca-Scores\\datasets\\common_data\n",
      "âœ… Found file: c:\\Users\\juliu\\OneDrive\\Desktop\\Projects\\Soca-Scores\\datasets\\common_data\\english_league_data_url.csv\n",
      "ğŸ“Š File loaded successfully! Shape: (15, 3)\n",
      "ğŸ“Š BASIC DATA OVERVIEW\n",
      "==================================================\n",
      "ğŸ“ Dataset shape: 15 rows Ã— 3 columns\n",
      "\n",
      "ğŸ“‹ First 5 rows:\n",
      "   Season_ID                                        Seasons_url  \\\n",
      "0  2024-2025  https://www.football-data.co.uk/mmz4281/2425/E...   \n",
      "1  2023-2024  https://www.football-data.co.uk/mmz4281/2324/E...   \n",
      "2  2022-2023  https://www.football-data.co.uk/mmz4281/2223/E...   \n",
      "3  2021-2022  https://www.football-data.co.uk/mmz4281/2122/E...   \n",
      "4  2020-2021  https://www.football-data.co.uk/mmz4281/2021/E...   \n",
      "\n",
      "  Competition_name  \n",
      "0              EPL  \n",
      "1              EPL  \n",
      "2              EPL  \n",
      "3              EPL  \n",
      "4              EPL  \n",
      "\n",
      "ğŸ·ï¸ Column names:\n",
      "   1. Season_ID\n",
      "   2. Seasons_url\n",
      "   3. Competition_name\n",
      "ğŸ” COLUMN INFORMATION\n",
      "==================================================\n",
      "ğŸ“Š Data types:\n",
      "   â€¢ Season_ID: object\n",
      "   â€¢ Seasons_url: object\n",
      "   â€¢ Competition_name: object\n",
      "\n",
      "ğŸ“ˆ Basic statistics:\n",
      "   â€¢ Total columns: 3\n",
      "   â€¢ Total rows: 15\n",
      "   â€¢ Memory usage: 3.21 KB\n",
      "ğŸ“‹ COLUMN CONTENT EXPLORATION\n",
      "==================================================\n",
      "\n",
      "ğŸ·ï¸ Column: 'Season_ID'\n",
      "   ğŸ“Š Data type: object\n",
      "   ğŸ”¢ Non-null count: 15/15\n",
      "   ğŸ”— Unique values: 15\n",
      "   ğŸ“„ Sample values:\n",
      "      1. 2024-2025\n",
      "      2. 2023-2024\n",
      "      3. 2022-2023\n",
      "      4. 2021-2022\n",
      "      5. 2020-2021\n",
      "      ... and 10 more unique values\n",
      "------------------------------\n",
      "\n",
      "ğŸ·ï¸ Column: 'Seasons_url'\n",
      "   ğŸ“Š Data type: object\n",
      "   ğŸ”¢ Non-null count: 15/15\n",
      "   ğŸ”— Unique values: 15\n",
      "   ğŸ“„ Sample values:\n",
      "      1. https://www.football-data.co.uk/mmz4281/2425/E0.csv\n",
      "      2. https://www.football-data.co.uk/mmz4281/2324/E0.csv\n",
      "      3. https://www.football-data.co.uk/mmz4281/2223/E0.csv\n",
      "      4. https://www.football-data.co.uk/mmz4281/2122/E0.csv\n",
      "      5. https://www.football-data.co.uk/mmz4281/2021/E0.csv\n",
      "      ... and 10 more unique values\n",
      "------------------------------\n",
      "\n",
      "ğŸ·ï¸ Column: 'Competition_name'\n",
      "   ğŸ“Š Data type: object\n",
      "   ğŸ”¢ Non-null count: 15/15\n",
      "   ğŸ”— Unique values: 1\n",
      "   ğŸ“„ Sample values:\n",
      "      1. EPL\n",
      "------------------------------\n",
      "â“ MISSING VALUES CHECK\n",
      "==================================================\n",
      "ğŸ“Š Missing values per column:\n",
      "   âœ… Season_ID: 0 (0.0%)\n",
      "   âœ… Seasons_url: 0 (0.0%)\n",
      "   âœ… Competition_name: 0 (0.0%)\n",
      "ğŸ”— URL-LIKE DATA DETECTION\n",
      "==================================================\n",
      "ğŸ·ï¸ Season_ID:\n",
      "   ğŸ”— URL-like entries: 0/15 (0.0%)\n",
      "\n",
      "ğŸ·ï¸ Seasons_url:\n",
      "   ğŸ”— URL-like entries: 15/15 (100.0%)\n",
      "   âœ… This looks like a URL column!\n",
      "   ğŸ“„ Examples:\n",
      "      1. https://www.football-data.co.uk/mmz4281/2425/E0.csv\n",
      "      2. https://www.football-data.co.uk/mmz4281/2324/E0.csv\n",
      "      3. https://www.football-data.co.uk/mmz4281/2223/E0.csv\n",
      "\n",
      "ğŸ·ï¸ Competition_name:\n",
      "   ğŸ”— URL-like entries: 0/15 (0.0%)\n",
      "\n",
      "ğŸ¯ Found 1 URL column(s): ['Seasons_url']\n",
      "ğŸ’¾ SAVING EXPLORATION RESULTS\n",
      "==================================================\n",
      "ğŸ“‹ Exploration Summary:\n",
      "   ğŸ“„ File: english_league_data_url.csv\n",
      "   ğŸ“Š Shape: (15, 3)\n",
      "   ğŸ·ï¸ Columns: 3\n",
      "\n",
      "âœ… Data loaded and ready for next steps!\n",
      "   Variable 'urls' contains your data\n",
      "   Shape: (15, 3)\n",
      "   Ready to proceed to next notebook/analysis\n",
      "ğŸ¯ QUICK DATA PEEK FOR NEXT STEPS\n",
      "========================================\n",
      "ğŸ“Š Dataset sample (first 3 rows, all columns):\n",
      "   Season_ID                                          Seasons_url Competition_name\n",
      "0  2024-2025  https://www.football-data.co.uk/mmz4281/2425/E0.csv              EPL\n",
      "1  2023-2024  https://www.football-data.co.uk/mmz4281/2324/E0.csv              EPL\n",
      "2  2022-2023  https://www.football-data.co.uk/mmz4281/2223/E0.csv              EPL\n",
      "\n",
      "ğŸ“ What this data might be useful for:\n",
      "   â€¢ Number of entries: 15\n",
      "   â€¢ Columns available: 3\n",
      "   â€¢ Can guide our next data collection steps\n",
      "\n",
      "ğŸš€ Ready to move to the next step!\n"
     ]
    }
   ],
   "source": [
    "#==========REFERENCE==========#\n",
    "\n",
    "# %% [markdown]\n",
    "# # ğŸ† Simple Premier League Data Exploration\n",
    "# \n",
    "# Starting simple - let's explore the CSV file we have and understand its structure.\n",
    "\n",
    "# %%\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"âœ… Imports completed!\")\n",
    "\n",
    "# %%\n",
    "# Define the data directories (your existing code)\n",
    "PROJECT_ROOT = Path().absolute().parent.parent\n",
    "DATA_DIR = PROJECT_ROOT / \"datasets\" \n",
    "COMMON_DATA_DIR = DATA_DIR / \"common_data\"\n",
    "\n",
    "print(f\"ğŸ“ Project root: {PROJECT_ROOT}\")\n",
    "print(f\"ğŸ“ Data directory: {DATA_DIR}\")\n",
    "print(f\"ğŸ“ Common data directory: {COMMON_DATA_DIR}\")\n",
    "\n",
    "# %%\n",
    "# Load the CSV file \n",
    "csv_file = COMMON_DATA_DIR / \"english_league_data_url.csv\"\n",
    "\n",
    "# Check if file exists\n",
    "if csv_file.exists():\n",
    "    print(f\"âœ… Found file: {csv_file}\")\n",
    "    \n",
    "    # Load the CSV file\n",
    "    urls = pd.read_csv(csv_file)\n",
    "    print(f\"ğŸ“Š File loaded successfully! Shape: {urls.shape}\")\n",
    "else:\n",
    "    print(f\"âŒ File not found: {csv_file}\")\n",
    "    # Let's check what files are actually in that directory\n",
    "    print(f\"ğŸ“ Files in {COMMON_DATA_DIR}:\")\n",
    "    if COMMON_DATA_DIR.exists():\n",
    "        for file in COMMON_DATA_DIR.iterdir():\n",
    "            print(f\"   â€¢ {file.name}\")\n",
    "    else:\n",
    "        print(\"   Directory doesn't exist!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ğŸ” Step 1: Basic Data Overview\n",
    "\n",
    "# %%\n",
    "# Let's look at the basic structure\n",
    "print(\"ğŸ“Š BASIC DATA OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'urls' in locals():\n",
    "    # Show the shape\n",
    "    print(f\"ğŸ“ Dataset shape: {urls.shape[0]} rows Ã— {urls.shape[1]} columns\")\n",
    "    \n",
    "    # Show the first few rows\n",
    "    print(f\"\\nğŸ“‹ First 5 rows:\")\n",
    "    print(urls.head())\n",
    "    \n",
    "    # Show column names\n",
    "    print(f\"\\nğŸ·ï¸ Column names:\")\n",
    "    for i, col in enumerate(urls.columns, 1):\n",
    "        print(f\"   {i}. {col}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ğŸ” Step 2: Column Information\n",
    "\n",
    "# %%\n",
    "# Extract detailed column information\n",
    "print(\"ğŸ” COLUMN INFORMATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'urls' in locals():\n",
    "    # Data types\n",
    "    print(\"ğŸ“Š Data types:\")\n",
    "    for col, dtype in urls.dtypes.items():\n",
    "        print(f\"   â€¢ {col}: {dtype}\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\nğŸ“ˆ Basic statistics:\")\n",
    "    print(f\"   â€¢ Total columns: {len(urls.columns)}\")\n",
    "    print(f\"   â€¢ Total rows: {len(urls)}\")\n",
    "    print(f\"   â€¢ Memory usage: {urls.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ğŸ” Step 3: Data Content Exploration\n",
    "\n",
    "# %%\n",
    "# Look at the actual content of each column\n",
    "print(\"ğŸ“‹ COLUMN CONTENT EXPLORATION\") \n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'urls' in locals():\n",
    "    for col in urls.columns:\n",
    "        print(f\"\\nğŸ·ï¸ Column: '{col}'\")\n",
    "        print(f\"   ğŸ“Š Data type: {urls[col].dtype}\")\n",
    "        print(f\"   ğŸ”¢ Non-null count: {urls[col].count()}/{len(urls)}\")\n",
    "        print(f\"   ğŸ”— Unique values: {urls[col].nunique()}\")\n",
    "        \n",
    "        # Show sample values\n",
    "        non_null_values = urls[col].dropna()\n",
    "        if len(non_null_values) > 0:\n",
    "            print(f\"   ğŸ“„ Sample values:\")\n",
    "            # Show up to 5 unique sample values\n",
    "            sample_values = non_null_values.unique()[:5]\n",
    "            for i, value in enumerate(sample_values, 1):\n",
    "                # Truncate long values\n",
    "                value_str = str(value)\n",
    "                if len(value_str) > 60:\n",
    "                    value_str = value_str[:60] + \"...\"\n",
    "                print(f\"      {i}. {value_str}\")\n",
    "            \n",
    "            if len(non_null_values.unique()) > 5:\n",
    "                print(f\"      ... and {len(non_null_values.unique()) - 5} more unique values\")\n",
    "        \n",
    "        print(\"-\" * 30)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ğŸ” Step 4: Missing Values Check\n",
    "\n",
    "# %%\n",
    "# Check for missing values\n",
    "print(\"â“ MISSING VALUES CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'urls' in locals():\n",
    "    missing_values = urls.isnull().sum()\n",
    "    total_rows = len(urls)\n",
    "    \n",
    "    print(f\"ğŸ“Š Missing values per column:\")\n",
    "    for col in urls.columns:\n",
    "        missing_count = missing_values[col]\n",
    "        missing_pct = (missing_count / total_rows) * 100\n",
    "        \n",
    "        if missing_count > 0:\n",
    "            print(f\"   âŒ {col}: {missing_count} ({missing_pct:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"   âœ… {col}: 0 (0.0%)\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ğŸ” Step 5: What Looks Like URLs?\n",
    "\n",
    "# %%\n",
    "# Since this is called \"english_league_data_url\", let's find URL-like columns\n",
    "print(\"ğŸ”— URL-LIKE DATA DETECTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'urls' in locals():\n",
    "    url_columns = []\n",
    "    \n",
    "    for col in urls.columns:\n",
    "        # Check if column contains URL-like strings\n",
    "        sample_values = urls[col].dropna().astype(str)\n",
    "        \n",
    "        if len(sample_values) > 0:\n",
    "            # Look for http/https or www patterns\n",
    "            url_like_count = sample_values.str.contains(r'http|www|\\.com|\\.co\\.uk|\\.csv', case=False, na=False).sum()\n",
    "            url_like_pct = (url_like_count / len(sample_values)) * 100\n",
    "            \n",
    "            print(f\"ğŸ·ï¸ {col}:\")\n",
    "            print(f\"   ğŸ”— URL-like entries: {url_like_count}/{len(sample_values)} ({url_like_pct:.1f}%)\")\n",
    "            \n",
    "            if url_like_pct > 50:  # If more than 50% look like URLs\n",
    "                url_columns.append(col)\n",
    "                print(f\"   âœ… This looks like a URL column!\")\n",
    "                \n",
    "                # Show some examples\n",
    "                url_examples = sample_values[sample_values.str.contains(r'http|www|\\.com|\\.co\\.uk|\\.csv', case=False, na=False)].head(3)\n",
    "                print(f\"   ğŸ“„ Examples:\")\n",
    "                for i, url in enumerate(url_examples, 1):\n",
    "                    print(f\"      {i}. {url}\")\n",
    "            \n",
    "            print()\n",
    "    \n",
    "    if url_columns:\n",
    "        print(f\"ğŸ¯ Found {len(url_columns)} URL column(s): {url_columns}\")\n",
    "    else:\n",
    "        print(\"ğŸ¤” No obvious URL columns found\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ğŸ’¾ Step 6: Save What We Found\n",
    "\n",
    "# %%\n",
    "# Save our exploration results\n",
    "print(\"ğŸ’¾ SAVING EXPLORATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'urls' in locals():\n",
    "    # Create a simple summary\n",
    "    exploration_summary = {\n",
    "        'file_name': csv_file.name,\n",
    "        'shape': urls.shape,\n",
    "        'columns': list(urls.columns),\n",
    "        'data_types': urls.dtypes.to_dict(),\n",
    "        'missing_values': urls.isnull().sum().to_dict()\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ“‹ Exploration Summary:\")\n",
    "    print(f\"   ğŸ“„ File: {exploration_summary['file_name']}\")\n",
    "    print(f\"   ğŸ“Š Shape: {exploration_summary['shape']}\")\n",
    "    print(f\"   ğŸ·ï¸ Columns: {len(exploration_summary['columns'])}\")\n",
    "    \n",
    "    # Save the original data info for next steps\n",
    "    print(f\"\\nâœ… Data loaded and ready for next steps!\")\n",
    "    print(f\"   Variable 'urls' contains your data\")\n",
    "    print(f\"   Shape: {urls.shape}\")\n",
    "    print(f\"   Ready to proceed to next notebook/analysis\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ğŸ¯ Next Steps\n",
    "# \n",
    "# Now that we've explored the basic structure:\n",
    "# 1. âœ… **Loaded the Excel file successfully**\n",
    "# 2. âœ… **Understood the column structure** \n",
    "# 3. âœ… **Checked for missing values**\n",
    "# 4. âœ… **Identified potential URL columns**\n",
    "# \n",
    "# **What's Next?**\n",
    "# - If we found URLs, we can use them to download actual match data\n",
    "# - If this contains team/league info, we can use it for data organization\n",
    "# - Move to feature engineering once we understand the data structure\n",
    "\n",
    "# %%\n",
    "# Quick peek at what we have for planning next steps\n",
    "if 'urls' in locals():\n",
    "    print(\"ğŸ¯ QUICK DATA PEEK FOR NEXT STEPS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"ğŸ“Š Dataset sample (first 3 rows, all columns):\")\n",
    "    print(urls.head(3).to_string())\n",
    "    \n",
    "    print(f\"\\nğŸ“ What this data might be useful for:\")\n",
    "    print(f\"   â€¢ Number of entries: {len(urls)}\")\n",
    "    print(f\"   â€¢ Columns available: {len(urls.columns)}\")\n",
    "    print(f\"   â€¢ Can guide our next data collection steps\")\n",
    "\n",
    "print(f\"\\nğŸš€ Ready to move to the next step!\")\n",
    "\n",
    "# %%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
