{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "282c8687",
   "metadata": {},
   "source": [
    "**DATA INGESTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74556d37",
   "metadata": {},
   "source": [
    "1. Go through the url workbook\n",
    "2. Ascertain that the number of rows and columns are as expected\n",
    "3. Implement a customer logger\n",
    "4. Load the pd.concat dataset back into the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed723d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Project root: c:\\Users\\juliu\\OneDrive\\Desktop\\Projects\\Soca-Scores\n",
      "📁 Data directory: c:\\Users\\juliu\\OneDrive\\Desktop\\Projects\\Soca-Scores\\datasets\n",
      "📁 Common data directory: c:\\Users\\juliu\\OneDrive\\Desktop\\Projects\\Soca-Scores\\datasets\\common_data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== Defining the data directories ====#\n",
    "PROJECT_ROOT = Path().absolute().parent.parent\n",
    "DATA_DIR = PROJECT_ROOT / \"datasets\" \n",
    "COMMON_DATA_DIR = DATA_DIR / \"common_data\"\n",
    "\n",
    "print(f\"📁 Project root: {PROJECT_ROOT}\")\n",
    "print(f\"📁 Data directory: {DATA_DIR}\")\n",
    "print(f\"📁 Common data directory: {COMMON_DATA_DIR}\")\n",
    "\n",
    "# == Dont forget the clear from the github commits == #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ca06b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found file: c:\\Users\\juliu\\OneDrive\\Desktop\\Projects\\Soca-Scores\\datasets\\common_data\\english_league_data_url.csv\n",
      "📊File has been loaded successfully. The url dataset has 15 rows and 3 column\n"
     ]
    }
   ],
   "source": [
    "# ===== Loading the url datasets ==== #\n",
    "\n",
    "csv_file = COMMON_DATA_DIR / \"english_league_data_url.csv\"\n",
    "try:\n",
    "    if csv_file.exists():\n",
    "        print(f\"✅ Found file: {csv_file}\")\n",
    "        #proceed to load it\n",
    "        urls = pd.read_csv(csv_file)\n",
    "        print(f'📊File has been loaded successfully. The url dataset has {urls.shape[0]} rows and {urls.shape[1]} columns')\n",
    "    else:\n",
    "        print(f\"❌ File not found: {csv_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Operation unsuccessful. Reason : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee9a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting enhanced data download...\n",
      "============================================================\n",
      "\n",
      "📥 Processing: 2024-2025 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/2425/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 122 (including metadata)\n",
      "\n",
      "📥 Processing: 2023-2024 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/2324/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 108 (including metadata)\n",
      "\n",
      "📥 Processing: 2022-2023 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/2223/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 108 (including metadata)\n",
      "\n",
      "📥 Processing: 2021-2022 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/2122/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 108 (including metadata)\n",
      "\n",
      "📥 Processing: 2020-2021 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/2021/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 108 (including metadata)\n",
      "\n",
      "📥 Processing: 2019-2020 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/1920/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 108 (including metadata)\n",
      "\n",
      "📥 Processing: 2018-2019 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/1819/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 64 (including metadata)\n",
      "\n",
      "📥 Processing: 2017-2018 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/1718/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 67 (including metadata)\n",
      "\n",
      "📥 Processing: 2016-2017 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/1617/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 67 (including metadata)\n",
      "\n",
      "📥 Processing: 2015-2016 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/1516/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 67 (including metadata)\n",
      "\n",
      "📥 Processing: 2014-2015 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/1415/E0.csv\n",
      "   ✅ Success: 381 matches loaded\n",
      "   📊 Columns: 70 (including metadata)\n",
      "\n",
      "📥 Processing: 2013-2014 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/1314/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 70 (including metadata)\n",
      "\n",
      "📥 Processing: 2012-2013 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/1213/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 76 (including metadata)\n",
      "\n",
      "📥 Processing: 2011-2012 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/1112/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 73 (including metadata)\n",
      "\n",
      "📥 Processing: 2010-2011 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/1011/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 73 (including metadata)\n"
     ]
    }
   ],
   "source": [
    "# ===== Adding season_id and competition_name to datasets ==== #\n",
    "\n",
    "if urls is not None:\n",
    "    print(f\"\\n🚀 Starting enhanced data download...\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    dataframes = []\n",
    "\n",
    "        #iterate through each row in the urls datasets\n",
    "    for index,row in urls.iterrows():\n",
    "        season_id = row[\"Season_ID\"]\n",
    "        seasons_url = row[\"Seasons_url\"]\n",
    "        competition_name = row[\"Competition_name\"]\n",
    "        print(f\"\\n📥 Processing: {season_id} - {competition_name}\")\n",
    "        print(f\"🔗 URL: {seasons_url}\")\n",
    "\n",
    "        try:\n",
    "            epl_data = pd.read_csv(seasons_url)\n",
    "\n",
    "            #Add metadata columns before adding\n",
    "            epl_data[\"season_id\"] = season_id\n",
    "            epl_data[\"competition_name\"] = competition_name\n",
    "            \n",
    "            dataframes.append(epl_data)\n",
    "            print(f\"   ✅ Success: {epl_data.shape[0]} matches loaded\")\n",
    "\n",
    "            #Possible tests -- checking if 380 matches are loaded\n",
    "\n",
    "            print(f\"   📊 Columns: {epl_data.shape[1]} (including metadata)\")\n",
    "        except Exception as e:\n",
    "            print(f\" ❌ Error reading data from {seasons_url}: {e}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14878f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Combining all datasets...\n",
      "\n",
      "🎉 Data successfully ingested and concatenated!\n",
      "📊 Final dataset shape: (5701, 167)\n",
      "\n",
      "📈 Summary by Season and Competition:\n",
      "season_id competition_name  match_count\n",
      "2010-2011              EPL          380\n",
      "2011-2012              EPL          380\n",
      "2012-2013              EPL          380\n",
      "2013-2014              EPL          380\n",
      "2014-2015              EPL          381\n",
      "2015-2016              EPL          380\n",
      "2016-2017              EPL          380\n",
      "2017-2018              EPL          380\n",
      "2018-2019              EPL          380\n",
      "2019-2020              EPL          380\n",
      "2020-2021              EPL          380\n",
      "2021-2022              EPL          380\n",
      "2022-2023              EPL          380\n",
      "2023-2024              EPL          380\n",
      "2024-2025              EPL          380\n",
      "\n",
      "📋 Sample of final dataset (with metadata):\n",
      "season_id competition_name       Date      HomeTeam       AwayTeam  FTHG  FTAG FTR\n",
      "2024-2025              EPL 16/08/2024    Man United         Fulham   1.0   0.0   H\n",
      "2024-2025              EPL 17/08/2024       Ipswich      Liverpool   0.0   2.0   A\n",
      "2024-2025              EPL 17/08/2024       Arsenal         Wolves   2.0   0.0   H\n",
      "2024-2025              EPL 17/08/2024       Everton       Brighton   0.0   3.0   A\n",
      "2024-2025              EPL 17/08/2024     Newcastle    Southampton   1.0   0.0   H\n",
      "2024-2025              EPL 17/08/2024 Nott'm Forest    Bournemouth   1.0   1.0   D\n",
      "2024-2025              EPL 17/08/2024      West Ham    Aston Villa   1.0   2.0   A\n",
      "2024-2025              EPL 18/08/2024     Brentford Crystal Palace   2.0   1.0   H\n",
      "2024-2025              EPL 18/08/2024       Chelsea       Man City   0.0   2.0   A\n",
      "2024-2025              EPL 19/08/2024     Leicester      Tottenham   1.0   1.0   D\n",
      "\n",
      "🔍 Data Validation:\n",
      "   📊 Total matches: 5,701\n",
      "   🏆 Unique seasons: 15\n",
      "   🏟️ Unique competitions: 1\n",
      "\n",
      "💾 Enhanced dataset saved to: c:\\Users\\juliu\\OneDrive\\Desktop\\Projects\\Soca-Scores\\datasets\\ingested\\combined_football_data.csv\n"
     ]
    }
   ],
   "source": [
    "#Combine with the block above\n",
    "\n",
    "if dataframes:\n",
    "    print(f\"\\n🔄 Combining all datasets...\")\n",
    "    final_df = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"\\n🎉 Data successfully ingested and concatenated!\")\n",
    "    print(f\"📊 Final dataset shape: {final_df.shape}\")\n",
    "\n",
    "    #Show summary by season and competition\n",
    "    print(f\"\\n📈 Summary by Season and Competition:\")\n",
    "    summary = final_df.groupby(['season_id', 'competition_name']).size().reset_index(name='match_count')\n",
    "    print(summary.to_string(index=False))\n",
    "\n",
    "    #Deleting the Division Column\n",
    "    final_df = final_df.drop('Div',axis=1)\n",
    "    \n",
    "    # Show sample of the final dataset with metadata\n",
    "    print(f\"\\n📋 Sample of final dataset (with metadata):\")\n",
    "    display_columns = ['season_id', 'competition_name', 'Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR']\n",
    "    available_columns = [col for col in display_columns if col in final_df.columns]\n",
    "    print(final_df[available_columns].head(10).to_string(index=False))\n",
    "\n",
    "\n",
    "    #Quick Validation\n",
    "    print(f\"\\n🔍 Data Validation:\")\n",
    "    print(f\"   📊 Total matches: {len(final_df):,}\")\n",
    "    print(f\"   🏆 Unique seasons: {final_df['season_id'].nunique()}\")\n",
    "    print(f\"   🏟️ Unique competitions: {final_df['competition_name'].nunique()}\")\n",
    "\n",
    "\n",
    "    #Save the final dataset\n",
    "    output_file = DATA_DIR / \"ingested\" / \"combined_football_data.csv\"\n",
    "    output_file.parent.mkdir(exist_ok=True)\n",
    "    final_df.to_csv(output_file,index=False)\n",
    "    print(f\"\\n💾 Enhanced dataset saved to: {output_file}\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n❌ No dataframes were successfully loaded.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#else:\n",
    " #   print(f\"\\n❌ Cannot proceed - URLs dataset not loaded properly.\")\n",
    "\n",
    "\n",
    "\n",
    "#TO DO Add logger\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc584c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports completed!\n",
      "📁 Project root: c:\\Users\\juliu\\OneDrive\\Desktop\\Projects\\Soca-Scores\n",
      "📁 Data directory: c:\\Users\\juliu\\OneDrive\\Desktop\\Projects\\Soca-Scores\\datasets\n",
      "📁 Common data directory: c:\\Users\\juliu\\OneDrive\\Desktop\\Projects\\Soca-Scores\\datasets\\common_data\n",
      "✅ Found file: c:\\Users\\juliu\\OneDrive\\Desktop\\Projects\\Soca-Scores\\datasets\\common_data\\english_league_data_url.csv\n",
      "📊 File loaded successfully! Shape: (15, 3)\n",
      "📊 BASIC DATA OVERVIEW\n",
      "==================================================\n",
      "📐 Dataset shape: 15 rows × 3 columns\n",
      "\n",
      "📋 First 5 rows:\n",
      "   Season_ID                                        Seasons_url  \\\n",
      "0  2024-2025  https://www.football-data.co.uk/mmz4281/2425/E...   \n",
      "1  2023-2024  https://www.football-data.co.uk/mmz4281/2324/E...   \n",
      "2  2022-2023  https://www.football-data.co.uk/mmz4281/2223/E...   \n",
      "3  2021-2022  https://www.football-data.co.uk/mmz4281/2122/E...   \n",
      "4  2020-2021  https://www.football-data.co.uk/mmz4281/2021/E...   \n",
      "\n",
      "  Competition_name  \n",
      "0              EPL  \n",
      "1              EPL  \n",
      "2              EPL  \n",
      "3              EPL  \n",
      "4              EPL  \n",
      "\n",
      "🏷️ Column names:\n",
      "   1. Season_ID\n",
      "   2. Seasons_url\n",
      "   3. Competition_name\n",
      "🔍 COLUMN INFORMATION\n",
      "==================================================\n",
      "📊 Data types:\n",
      "   • Season_ID: object\n",
      "   • Seasons_url: object\n",
      "   • Competition_name: object\n",
      "\n",
      "📈 Basic statistics:\n",
      "   • Total columns: 3\n",
      "   • Total rows: 15\n",
      "   • Memory usage: 3.21 KB\n",
      "📋 COLUMN CONTENT EXPLORATION\n",
      "==================================================\n",
      "\n",
      "🏷️ Column: 'Season_ID'\n",
      "   📊 Data type: object\n",
      "   🔢 Non-null count: 15/15\n",
      "   🔗 Unique values: 15\n",
      "   📄 Sample values:\n",
      "      1. 2024-2025\n",
      "      2. 2023-2024\n",
      "      3. 2022-2023\n",
      "      4. 2021-2022\n",
      "      5. 2020-2021\n",
      "      ... and 10 more unique values\n",
      "------------------------------\n",
      "\n",
      "🏷️ Column: 'Seasons_url'\n",
      "   📊 Data type: object\n",
      "   🔢 Non-null count: 15/15\n",
      "   🔗 Unique values: 15\n",
      "   📄 Sample values:\n",
      "      1. https://www.football-data.co.uk/mmz4281/2425/E0.csv\n",
      "      2. https://www.football-data.co.uk/mmz4281/2324/E0.csv\n",
      "      3. https://www.football-data.co.uk/mmz4281/2223/E0.csv\n",
      "      4. https://www.football-data.co.uk/mmz4281/2122/E0.csv\n",
      "      5. https://www.football-data.co.uk/mmz4281/2021/E0.csv\n",
      "      ... and 10 more unique values\n",
      "------------------------------\n",
      "\n",
      "🏷️ Column: 'Competition_name'\n",
      "   📊 Data type: object\n",
      "   🔢 Non-null count: 15/15\n",
      "   🔗 Unique values: 1\n",
      "   📄 Sample values:\n",
      "      1. EPL\n",
      "------------------------------\n",
      "❓ MISSING VALUES CHECK\n",
      "==================================================\n",
      "📊 Missing values per column:\n",
      "   ✅ Season_ID: 0 (0.0%)\n",
      "   ✅ Seasons_url: 0 (0.0%)\n",
      "   ✅ Competition_name: 0 (0.0%)\n",
      "🔗 URL-LIKE DATA DETECTION\n",
      "==================================================\n",
      "🏷️ Season_ID:\n",
      "   🔗 URL-like entries: 0/15 (0.0%)\n",
      "\n",
      "🏷️ Seasons_url:\n",
      "   🔗 URL-like entries: 15/15 (100.0%)\n",
      "   ✅ This looks like a URL column!\n",
      "   📄 Examples:\n",
      "      1. https://www.football-data.co.uk/mmz4281/2425/E0.csv\n",
      "      2. https://www.football-data.co.uk/mmz4281/2324/E0.csv\n",
      "      3. https://www.football-data.co.uk/mmz4281/2223/E0.csv\n",
      "\n",
      "🏷️ Competition_name:\n",
      "   🔗 URL-like entries: 0/15 (0.0%)\n",
      "\n",
      "🎯 Found 1 URL column(s): ['Seasons_url']\n",
      "💾 SAVING EXPLORATION RESULTS\n",
      "==================================================\n",
      "📋 Exploration Summary:\n",
      "   📄 File: english_league_data_url.csv\n",
      "   📊 Shape: (15, 3)\n",
      "   🏷️ Columns: 3\n",
      "\n",
      "✅ Data loaded and ready for next steps!\n",
      "   Variable 'urls' contains your data\n",
      "   Shape: (15, 3)\n",
      "   Ready to proceed to next notebook/analysis\n",
      "🎯 QUICK DATA PEEK FOR NEXT STEPS\n",
      "========================================\n",
      "📊 Dataset sample (first 3 rows, all columns):\n",
      "   Season_ID                                          Seasons_url Competition_name\n",
      "0  2024-2025  https://www.football-data.co.uk/mmz4281/2425/E0.csv              EPL\n",
      "1  2023-2024  https://www.football-data.co.uk/mmz4281/2324/E0.csv              EPL\n",
      "2  2022-2023  https://www.football-data.co.uk/mmz4281/2223/E0.csv              EPL\n",
      "\n",
      "📝 What this data might be useful for:\n",
      "   • Number of entries: 15\n",
      "   • Columns available: 3\n",
      "   • Can guide our next data collection steps\n",
      "\n",
      "🚀 Ready to move to the next step!\n"
     ]
    }
   ],
   "source": [
    "#==========REFERENCE==========#\n",
    "\n",
    "# %% [markdown]\n",
    "# # 🏆 Simple Premier League Data Exploration\n",
    "# \n",
    "# Starting simple - let's explore the CSV file we have and understand its structure.\n",
    "\n",
    "# %%\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"✅ Imports completed!\")\n",
    "\n",
    "# %%\n",
    "# Define the data directories (your existing code)\n",
    "PROJECT_ROOT = Path().absolute().parent.parent\n",
    "DATA_DIR = PROJECT_ROOT / \"datasets\" \n",
    "COMMON_DATA_DIR = DATA_DIR / \"common_data\"\n",
    "\n",
    "print(f\"📁 Project root: {PROJECT_ROOT}\")\n",
    "print(f\"📁 Data directory: {DATA_DIR}\")\n",
    "print(f\"📁 Common data directory: {COMMON_DATA_DIR}\")\n",
    "\n",
    "# %%\n",
    "# Load the CSV file \n",
    "csv_file = COMMON_DATA_DIR / \"english_league_data_url.csv\"\n",
    "\n",
    "# Check if file exists\n",
    "if csv_file.exists():\n",
    "    print(f\"✅ Found file: {csv_file}\")\n",
    "    \n",
    "    # Load the CSV file\n",
    "    urls = pd.read_csv(csv_file)\n",
    "    print(f\"📊 File loaded successfully! Shape: {urls.shape}\")\n",
    "else:\n",
    "    print(f\"❌ File not found: {csv_file}\")\n",
    "    # Let's check what files are actually in that directory\n",
    "    print(f\"📁 Files in {COMMON_DATA_DIR}:\")\n",
    "    if COMMON_DATA_DIR.exists():\n",
    "        for file in COMMON_DATA_DIR.iterdir():\n",
    "            print(f\"   • {file.name}\")\n",
    "    else:\n",
    "        print(\"   Directory doesn't exist!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 🔍 Step 1: Basic Data Overview\n",
    "\n",
    "# %%\n",
    "# Let's look at the basic structure\n",
    "print(\"📊 BASIC DATA OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'urls' in locals():\n",
    "    # Show the shape\n",
    "    print(f\"📐 Dataset shape: {urls.shape[0]} rows × {urls.shape[1]} columns\")\n",
    "    \n",
    "    # Show the first few rows\n",
    "    print(f\"\\n📋 First 5 rows:\")\n",
    "    print(urls.head())\n",
    "    \n",
    "    # Show column names\n",
    "    print(f\"\\n🏷️ Column names:\")\n",
    "    for i, col in enumerate(urls.columns, 1):\n",
    "        print(f\"   {i}. {col}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 🔍 Step 2: Column Information\n",
    "\n",
    "# %%\n",
    "# Extract detailed column information\n",
    "print(\"🔍 COLUMN INFORMATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'urls' in locals():\n",
    "    # Data types\n",
    "    print(\"📊 Data types:\")\n",
    "    for col, dtype in urls.dtypes.items():\n",
    "        print(f\"   • {col}: {dtype}\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\n📈 Basic statistics:\")\n",
    "    print(f\"   • Total columns: {len(urls.columns)}\")\n",
    "    print(f\"   • Total rows: {len(urls)}\")\n",
    "    print(f\"   • Memory usage: {urls.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 🔍 Step 3: Data Content Exploration\n",
    "\n",
    "# %%\n",
    "# Look at the actual content of each column\n",
    "print(\"📋 COLUMN CONTENT EXPLORATION\") \n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'urls' in locals():\n",
    "    for col in urls.columns:\n",
    "        print(f\"\\n🏷️ Column: '{col}'\")\n",
    "        print(f\"   📊 Data type: {urls[col].dtype}\")\n",
    "        print(f\"   🔢 Non-null count: {urls[col].count()}/{len(urls)}\")\n",
    "        print(f\"   🔗 Unique values: {urls[col].nunique()}\")\n",
    "        \n",
    "        # Show sample values\n",
    "        non_null_values = urls[col].dropna()\n",
    "        if len(non_null_values) > 0:\n",
    "            print(f\"   📄 Sample values:\")\n",
    "            # Show up to 5 unique sample values\n",
    "            sample_values = non_null_values.unique()[:5]\n",
    "            for i, value in enumerate(sample_values, 1):\n",
    "                # Truncate long values\n",
    "                value_str = str(value)\n",
    "                if len(value_str) > 60:\n",
    "                    value_str = value_str[:60] + \"...\"\n",
    "                print(f\"      {i}. {value_str}\")\n",
    "            \n",
    "            if len(non_null_values.unique()) > 5:\n",
    "                print(f\"      ... and {len(non_null_values.unique()) - 5} more unique values\")\n",
    "        \n",
    "        print(\"-\" * 30)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 🔍 Step 4: Missing Values Check\n",
    "\n",
    "# %%\n",
    "# Check for missing values\n",
    "print(\"❓ MISSING VALUES CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'urls' in locals():\n",
    "    missing_values = urls.isnull().sum()\n",
    "    total_rows = len(urls)\n",
    "    \n",
    "    print(f\"📊 Missing values per column:\")\n",
    "    for col in urls.columns:\n",
    "        missing_count = missing_values[col]\n",
    "        missing_pct = (missing_count / total_rows) * 100\n",
    "        \n",
    "        if missing_count > 0:\n",
    "            print(f\"   ❌ {col}: {missing_count} ({missing_pct:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"   ✅ {col}: 0 (0.0%)\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 🔍 Step 5: What Looks Like URLs?\n",
    "\n",
    "# %%\n",
    "# Since this is called \"english_league_data_url\", let's find URL-like columns\n",
    "print(\"🔗 URL-LIKE DATA DETECTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'urls' in locals():\n",
    "    url_columns = []\n",
    "    \n",
    "    for col in urls.columns:\n",
    "        # Check if column contains URL-like strings\n",
    "        sample_values = urls[col].dropna().astype(str)\n",
    "        \n",
    "        if len(sample_values) > 0:\n",
    "            # Look for http/https or www patterns\n",
    "            url_like_count = sample_values.str.contains(r'http|www|\\.com|\\.co\\.uk|\\.csv', case=False, na=False).sum()\n",
    "            url_like_pct = (url_like_count / len(sample_values)) * 100\n",
    "            \n",
    "            print(f\"🏷️ {col}:\")\n",
    "            print(f\"   🔗 URL-like entries: {url_like_count}/{len(sample_values)} ({url_like_pct:.1f}%)\")\n",
    "            \n",
    "            if url_like_pct > 50:  # If more than 50% look like URLs\n",
    "                url_columns.append(col)\n",
    "                print(f\"   ✅ This looks like a URL column!\")\n",
    "                \n",
    "                # Show some examples\n",
    "                url_examples = sample_values[sample_values.str.contains(r'http|www|\\.com|\\.co\\.uk|\\.csv', case=False, na=False)].head(3)\n",
    "                print(f\"   📄 Examples:\")\n",
    "                for i, url in enumerate(url_examples, 1):\n",
    "                    print(f\"      {i}. {url}\")\n",
    "            \n",
    "            print()\n",
    "    \n",
    "    if url_columns:\n",
    "        print(f\"🎯 Found {len(url_columns)} URL column(s): {url_columns}\")\n",
    "    else:\n",
    "        print(\"🤔 No obvious URL columns found\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 💾 Step 6: Save What We Found\n",
    "\n",
    "# %%\n",
    "# Save our exploration results\n",
    "print(\"💾 SAVING EXPLORATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'urls' in locals():\n",
    "    # Create a simple summary\n",
    "    exploration_summary = {\n",
    "        'file_name': csv_file.name,\n",
    "        'shape': urls.shape,\n",
    "        'columns': list(urls.columns),\n",
    "        'data_types': urls.dtypes.to_dict(),\n",
    "        'missing_values': urls.isnull().sum().to_dict()\n",
    "    }\n",
    "    \n",
    "    print(\"📋 Exploration Summary:\")\n",
    "    print(f\"   📄 File: {exploration_summary['file_name']}\")\n",
    "    print(f\"   📊 Shape: {exploration_summary['shape']}\")\n",
    "    print(f\"   🏷️ Columns: {len(exploration_summary['columns'])}\")\n",
    "    \n",
    "    # Save the original data info for next steps\n",
    "    print(f\"\\n✅ Data loaded and ready for next steps!\")\n",
    "    print(f\"   Variable 'urls' contains your data\")\n",
    "    print(f\"   Shape: {urls.shape}\")\n",
    "    print(f\"   Ready to proceed to next notebook/analysis\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 🎯 Next Steps\n",
    "# \n",
    "# Now that we've explored the basic structure:\n",
    "# 1. ✅ **Loaded the Excel file successfully**\n",
    "# 2. ✅ **Understood the column structure** \n",
    "# 3. ✅ **Checked for missing values**\n",
    "# 4. ✅ **Identified potential URL columns**\n",
    "# \n",
    "# **What's Next?**\n",
    "# - If we found URLs, we can use them to download actual match data\n",
    "# - If this contains team/league info, we can use it for data organization\n",
    "# - Move to feature engineering once we understand the data structure\n",
    "\n",
    "# %%\n",
    "# Quick peek at what we have for planning next steps\n",
    "if 'urls' in locals():\n",
    "    print(\"🎯 QUICK DATA PEEK FOR NEXT STEPS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"📊 Dataset sample (first 3 rows, all columns):\")\n",
    "    print(urls.head(3).to_string())\n",
    "    \n",
    "    print(f\"\\n📝 What this data might be useful for:\")\n",
    "    print(f\"   • Number of entries: {len(urls)}\")\n",
    "    print(f\"   • Columns available: {len(urls.columns)}\")\n",
    "    print(f\"   • Can guide our next data collection steps\")\n",
    "\n",
    "print(f\"\\n🚀 Ready to move to the next step!\")\n",
    "\n",
    "# %%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
