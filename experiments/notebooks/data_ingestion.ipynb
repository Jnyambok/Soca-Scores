{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "282c8687",
   "metadata": {},
   "source": [
    "**DATA INGESTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74556d37",
   "metadata": {},
   "source": [
    "1. Go through the url workbook\n",
    "2. Ascertain that the number of rows and columns are as expected\n",
    "3. Implement a customer logger\n",
    "4. Load the pd.concat dataset back into the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed723d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== Defining the data directories ====#\n",
    "PROJECT_ROOT = Path().absolute().parent.parent\n",
    "DATA_DIR = PROJECT_ROOT / \"datasets\" \n",
    "COMMON_DATA_DIR = DATA_DIR / \"common_data\"\n",
    "\n",
    "#print(f\"📁 Project root: {PROJECT_ROOT}\")\n",
    "#print(f\"📁 Data directory: {DATA_DIR}\")\n",
    "#print(f\"📁 Common data directory: {COMMON_DATA_DIR}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ca06b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Loading the url datasets ==== #\n",
    "\n",
    "csv_file = COMMON_DATA_DIR / \"english_league_data_url.csv\"\n",
    "try:\n",
    "    if csv_file.exists():\n",
    "        print(f\"✅ Found file: {csv_file}\")\n",
    "        #proceed to load it\n",
    "        urls = pd.read_csv(csv_file)\n",
    "        print(f'📊File has been loaded successfully. The url dataset has {urls.shape[0]} rows and {urls.shape[1]} columns')\n",
    "    else:\n",
    "        print(f\"❌ File not found: {csv_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Operation unsuccessful. Reason : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee9a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting enhanced data download...\n",
      "============================================================\n",
      "\n",
      "📥 Processing: 2024-2025 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/2425/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 122 (including metadata)\n",
      "\n",
      "📥 Processing: 2023-2024 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/2324/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 108 (including metadata)\n",
      "\n",
      "📥 Processing: 2022-2023 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/2223/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 108 (including metadata)\n",
      "\n",
      "📥 Processing: 2021-2022 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/2122/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 108 (including metadata)\n",
      "\n",
      "📥 Processing: 2020-2021 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/2021/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 108 (including metadata)\n",
      "\n",
      "📥 Processing: 2019-2020 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/1920/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 108 (including metadata)\n",
      "\n",
      "📥 Processing: 2018-2019 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/1819/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 64 (including metadata)\n",
      "\n",
      "📥 Processing: 2017-2018 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/1718/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 67 (including metadata)\n",
      "\n",
      "📥 Processing: 2016-2017 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/1617/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 67 (including metadata)\n",
      "\n",
      "📥 Processing: 2015-2016 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/1516/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 67 (including metadata)\n",
      "\n",
      "📥 Processing: 2014-2015 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/1415/E0.csv\n",
      "   ✅ Success: 381 matches loaded\n",
      "   📊 Columns: 70 (including metadata)\n",
      "\n",
      "📥 Processing: 2013-2014 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/1314/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 70 (including metadata)\n",
      "\n",
      "📥 Processing: 2012-2013 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/1213/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 76 (including metadata)\n",
      "\n",
      "📥 Processing: 2011-2012 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/1112/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 73 (including metadata)\n",
      "\n",
      "📥 Processing: 2010-2011 - EPL\n",
      "🔗 URL: https://www.football-data.co.uk/mmz4281/1011/E0.csv\n",
      "   ✅ Success: 380 matches loaded\n",
      "   📊 Columns: 73 (including metadata)\n"
     ]
    }
   ],
   "source": [
    "# ===== Adding season_id and competition_name to datasets ==== #\n",
    "\n",
    "if urls is not None:\n",
    "    print(f\"\\n🚀 Starting enhanced data download...\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    dataframes = []\n",
    "\n",
    "        #iterate through each row in the urls datasets\n",
    "    for index,row in urls.iterrows():\n",
    "        season_id = row[\"Season_ID\"]\n",
    "        seasons_url = row[\"Seasons_url\"]\n",
    "        competition_name = row[\"Competition_name\"]\n",
    "        print(f\"\\n📥 Processing: {season_id} - {competition_name}\")\n",
    "        print(f\"🔗 URL: {seasons_url}\")\n",
    "\n",
    "        try:\n",
    "            epl_data = pd.read_csv(seasons_url)\n",
    "\n",
    "            #Add metadata columns before adding\n",
    "            epl_data[\"season_id\"] = season_id\n",
    "            epl_data[\"competition_name\"] = competition_name\n",
    "            \n",
    "            dataframes.append(epl_data)\n",
    "            print(f\"   ✅ Success: {epl_data.shape[0]} matches loaded\")\n",
    "\n",
    "            #Possible tests -- checking if 380 matches are loaded\n",
    "\n",
    "            print(f\"   📊 Columns: {epl_data.shape[1]} (including metadata)\")\n",
    "        except Exception as e:\n",
    "            print(f\" ❌ Error reading data from {seasons_url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14878f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine with the block above\n",
    "\n",
    "if dataframes:\n",
    "    print(f\"\\n🔄 Combining all datasets...\")\n",
    "    final_df = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"\\n🎉 Data successfully ingested and concatenated!\")\n",
    "    print(f\"📊 Final dataset shape: {final_df.shape}\")\n",
    "\n",
    "    #Show summary by season and competition\n",
    "    print(f\"\\n📈 Summary by Season and Competition:\")\n",
    "    summary = final_df.groupby(['season_id', 'competition_name']).size().reset_index(name='match_count')\n",
    "    print(summary.to_string(index=False))\n",
    "\n",
    "    #Deleting the Division Column\n",
    "    final_df = final_df.drop('Div',axis=1)\n",
    "    \n",
    "    # Show sample of the final dataset with metadata\n",
    "    print(f\"\\n📋 Sample of final dataset (with metadata):\")\n",
    "    display_columns = ['season_id', 'competition_name', 'Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR']\n",
    "    available_columns = [col for col in display_columns if col in final_df.columns]\n",
    "    print(final_df[available_columns].head(10).to_string(index=False))\n",
    "\n",
    "\n",
    "    #Quick Validation\n",
    "    print(f\"\\n🔍 Data Validation:\")\n",
    "    print(f\"   📊 Total matches: {len(final_df):,}\")\n",
    "    print(f\"   🏆 Unique seasons: {final_df['season_id'].nunique()}\")\n",
    "    print(f\"   🏟️ Unique competitions: {final_df['competition_name'].nunique()}\")\n",
    "\n",
    "\n",
    "    #Save the final dataset\n",
    "    output_file = DATA_DIR / \"ingested\" / \"combined_football_data.csv\"\n",
    "    output_file.parent.mkdir(exist_ok=True)\n",
    "    final_df.to_csv(output_file,index=False)\n",
    "    print(f\"\\n💾 Enhanced dataset saved to: {output_file}\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n❌ No dataframes were successfully loaded.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========REFERENCE==========#\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"✅ Imports completed!\")\n",
    "\n",
    "\n",
    "PROJECT_ROOT = Path().absolute().parent.parent\n",
    "DATA_DIR = PROJECT_ROOT / \"datasets\" \n",
    "COMMON_DATA_DIR = DATA_DIR / \"common_data\"\n",
    "\n",
    "print(f\"📁 Project root: {PROJECT_ROOT}\")\n",
    "print(f\"📁 Data directory: {DATA_DIR}\")\n",
    "print(f\"📁 Common data directory: {COMMON_DATA_DIR}\")\n",
    "\n",
    "\n",
    "csv_file = COMMON_DATA_DIR / \"english_league_data_url.csv\"\n",
    "\n",
    "# Check if file exists\n",
    "if csv_file.exists():\n",
    "    print(f\"✅ Found file: {csv_file}\")\n",
    "    \n",
    "    # Load the CSV file\n",
    "    urls = pd.read_csv(csv_file)\n",
    "    print(f\"📊 File loaded successfully! Shape: {urls.shape}\")\n",
    "else:\n",
    "    print(f\"❌ File not found: {csv_file}\")\n",
    "    # Let's check what files are actually in that directory\n",
    "    print(f\"📁 Files in {COMMON_DATA_DIR}:\")\n",
    "    if COMMON_DATA_DIR.exists():\n",
    "        for file in COMMON_DATA_DIR.iterdir():\n",
    "            print(f\"   • {file.name}\")\n",
    "    else:\n",
    "        print(\"   Directory doesn't exist!\")\n",
    "\n",
    "\n",
    "print(\"📊 BASIC DATA OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'urls' in locals():\n",
    "    # Show the shape\n",
    "    print(f\"📐 Dataset shape: {urls.shape[0]} rows × {urls.shape[1]} columns\")\n",
    "    \n",
    "    # Show the first few rows\n",
    "    print(f\"\\n📋 First 5 rows:\")\n",
    "    print(urls.head())\n",
    "    \n",
    "    # Show column names\n",
    "    print(f\"\\n🏷️ Column names:\")\n",
    "    for i, col in enumerate(urls.columns, 1):\n",
    "        print(f\"   {i}. {col}\")\n",
    "\n",
    "\n",
    "print(\"🔍 COLUMN INFORMATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'urls' in locals():\n",
    "    # Data types\n",
    "    print(\"📊 Data types:\")\n",
    "    for col, dtype in urls.dtypes.items():\n",
    "        print(f\"   • {col}: {dtype}\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\n📈 Basic statistics:\")\n",
    "    print(f\"   • Total columns: {len(urls.columns)}\")\n",
    "    print(f\"   • Total rows: {len(urls)}\")\n",
    "    print(f\"   • Memory usage: {urls.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "print(\"📋 COLUMN CONTENT EXPLORATION\") \n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'urls' in locals():\n",
    "    for col in urls.columns:\n",
    "        print(f\"\\n🏷️ Column: '{col}'\")\n",
    "        print(f\"   📊 Data type: {urls[col].dtype}\")\n",
    "        print(f\"   🔢 Non-null count: {urls[col].count()}/{len(urls)}\")\n",
    "        print(f\"   🔗 Unique values: {urls[col].nunique()}\")\n",
    "        \n",
    "        # Show sample values\n",
    "        non_null_values = urls[col].dropna()\n",
    "        if len(non_null_values) > 0:\n",
    "            print(f\"   📄 Sample values:\")\n",
    "            # Show up to 5 unique sample values\n",
    "            sample_values = non_null_values.unique()[:5]\n",
    "            for i, value in enumerate(sample_values, 1):\n",
    "                # Truncate long values\n",
    "                value_str = str(value)\n",
    "                if len(value_str) > 60:\n",
    "                    value_str = value_str[:60] + \"...\"\n",
    "                print(f\"      {i}. {value_str}\")\n",
    "            \n",
    "            if len(non_null_values.unique()) > 5:\n",
    "                print(f\"      ... and {len(non_null_values.unique()) - 5} more unique values\")\n",
    "        \n",
    "        print(\"-\" * 30)\n",
    "\n",
    "print(\"❓ MISSING VALUES CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'urls' in locals():\n",
    "    missing_values = urls.isnull().sum()\n",
    "    total_rows = len(urls)\n",
    "    \n",
    "    print(f\"📊 Missing values per column:\")\n",
    "    for col in urls.columns:\n",
    "        missing_count = missing_values[col]\n",
    "        missing_pct = (missing_count / total_rows) * 100\n",
    "        \n",
    "        if missing_count > 0:\n",
    "            print(f\"   ❌ {col}: {missing_count} ({missing_pct:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"   ✅ {col}: 0 (0.0%)\")\n",
    "print(\"🔗 URL-LIKE DATA DETECTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'urls' in locals():\n",
    "    url_columns = []\n",
    "    \n",
    "    for col in urls.columns:\n",
    "        # Check if column contains URL-like strings\n",
    "        sample_values = urls[col].dropna().astype(str)\n",
    "        \n",
    "        if len(sample_values) > 0:\n",
    "            # Look for http/https or www patterns\n",
    "            url_like_count = sample_values.str.contains(r'http|www|\\.com|\\.co\\.uk|\\.csv', case=False, na=False).sum()\n",
    "            url_like_pct = (url_like_count / len(sample_values)) * 100\n",
    "            \n",
    "            print(f\"🏷️ {col}:\")\n",
    "            print(f\"   🔗 URL-like entries: {url_like_count}/{len(sample_values)} ({url_like_pct:.1f}%)\")\n",
    "            \n",
    "            if url_like_pct > 50:  # If more than 50% look like URLs\n",
    "                url_columns.append(col)\n",
    "                print(f\"   ✅ This looks like a URL column!\")\n",
    "                \n",
    "                # Show some examples\n",
    "                url_examples = sample_values[sample_values.str.contains(r'http|www|\\.com|\\.co\\.uk|\\.csv', case=False, na=False)].head(3)\n",
    "                print(f\"   📄 Examples:\")\n",
    "                for i, url in enumerate(url_examples, 1):\n",
    "                    print(f\"      {i}. {url}\")\n",
    "            \n",
    "            print()\n",
    "    \n",
    "    if url_columns:\n",
    "        print(f\"🎯 Found {len(url_columns)} URL column(s): {url_columns}\")\n",
    "    else:\n",
    "        print(\"🤔 No obvious URL columns found\")\n",
    "\n",
    "print(\"💾 SAVING EXPLORATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'urls' in locals():\n",
    "    # Create a simple summary\n",
    "    exploration_summary = {\n",
    "        'file_name': csv_file.name,\n",
    "        'shape': urls.shape,\n",
    "        'columns': list(urls.columns),\n",
    "        'data_types': urls.dtypes.to_dict(),\n",
    "        'missing_values': urls.isnull().sum().to_dict()\n",
    "    }\n",
    "    \n",
    "    print(\"📋 Exploration Summary:\")\n",
    "    print(f\"   📄 File: {exploration_summary['file_name']}\")\n",
    "    print(f\"   📊 Shape: {exploration_summary['shape']}\")\n",
    "    print(f\"   🏷️ Columns: {len(exploration_summary['columns'])}\")\n",
    "    \n",
    "    # Save the original data info for next steps\n",
    "    print(f\"\\n✅ Data loaded and ready for next steps!\")\n",
    "    print(f\"   Variable 'urls' contains your data\")\n",
    "    print(f\"   Shape: {urls.shape}\")\n",
    "    print(f\"   Ready to proceed to next notebook/analysis\")\n",
    "\n",
    "\n",
    "if 'urls' in locals():\n",
    "    print(\"🎯 QUICK DATA PEEK FOR NEXT STEPS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"📊 Dataset sample (first 3 rows, all columns):\")\n",
    "    print(urls.head(3).to_string())\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08715d23",
   "metadata": {},
   "source": [
    "**Implementing a logger** (Future works)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42acc8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-18 04:21:25.187\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mHello World\u001b[0m\n",
      "\u001b[32m2025-08-18 04:21:25.188\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[34m\u001b[1mHello World\u001b[0m\n",
      "\u001b[32m2025-08-18 04:21:25.189\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[32m\u001b[1mHello World\u001b[0m\n",
      "\u001b[32m2025-08-18 04:21:25.190\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[33m\u001b[1mHello World\u001b[0m\n",
      "\u001b[32m2025-08-18 04:21:25.192\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[31m\u001b[1mHello World\u001b[0m\n",
      "\u001b[32m2025-08-18 04:21:25.194\u001b[0m | \u001b[41m\u001b[1mCRITICAL\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[41m\u001b[1mHello World\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "logger.info(\"Hello World\")\n",
    "logger.trace(\"Hello World\")\n",
    "logger.debug(\"Hello World\")\n",
    "logger.success(\"Hello World\")\n",
    "logger.warning(\"Hello World\")\n",
    "logger.error(\"Hello World\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70b93539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-18T04:26:04.044740+0300\u001b[0m | ERROR | Hello World\n",
      "\u001b[32m2025-08-18T04:26:04.044740+0300\u001b[0m | ERROR | Hello World\n",
      "\u001b[32m2025-08-18T04:26:04.044740+0300\u001b[0m | ERROR | Hello World\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "logger.add(sys.stderr, format =\"<green>{time}</green> | {level} | {message}\")\n",
    "logger.error(\"Hello World\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
